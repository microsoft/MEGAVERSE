{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utils\n",
    "'''\n",
    "from collections import defaultdict as dd\n",
    "\n",
    "## LANG CLASS\n",
    "lang_class = {0:[],1:[],2:[],3:[],4:[],5:[],6:[]}#6 is code mix -- may be we can add this to respective 2nd lang grp\n",
    "class_lang = dd(list)\n",
    "f=open(\"dataset/lang_class_mapping.csv\");L=f.readlines();f.close()\n",
    "#lang-name, code, class\n",
    "for ele in L:\n",
    "    ele = ele.strip(\"\\n\").strip().split(\"\\t\")\n",
    "    if len(ele)>2:\n",
    "        lang_class[int(ele[2].strip())].append((ele[0].strip(),ele[1].strip()))\n",
    "        class_lang[ele[0].strip()].append(ele[2].strip())\n",
    "    else:\n",
    "        print(ele)\n",
    "       \n",
    "## NLP TRACK \n",
    "track_dataset = dd(list)\n",
    "dataset_track = dd(list)\n",
    "f=open(\"dataset/track_dataset_mapping.csv\");L=f.readlines();f.close()\n",
    "for ele in L[1:]:\n",
    "    ele = ele.strip(\"\\n\").split(\"\\t\")\n",
    "    if ele[1]:\n",
    "        track_dataset[ele[0]] = [e.strip() for e in ele[1].split(\",\")]\n",
    "        for data in ele[1].split(\",\"):\n",
    "            dataset_track[data.strip()] = ele[0]\n",
    "    else:\n",
    "        print(ele[0],\" has no dataset\")\n",
    "\n",
    "## DS to store results\n",
    "\n",
    "all_data = {0:dd(),1:dd(),2:dd(),3:dd(),4:dd(),5:dd(),6:dd()}\n",
    "for class_id in all_data.keys():\n",
    "    for lang in lang_class[class_id]:\n",
    "        lang=lang[0]\n",
    "        all_data[class_id][lang]=dd()\n",
    "        for track in track_dataset:\n",
    "            all_data[class_id][lang][track]=dd()\n",
    "            for dataset in track_dataset[track]:\n",
    "                all_data[class_id][lang][track][dataset] = 0.0\n",
    "        print(class_id, lang, all_data[class_id][lang])    \n",
    "\n",
    "\n",
    "def get_percent_change(lang_score, eng_score):\n",
    "    # if lang_score > eng_score:\n",
    "    #     cprint(\"Invalid assumption\", \"red\")\n",
    "    #     print(lang_score, eng_score, 100*np.double(eng_score - lang_score)/np.double(eng_score)) \n",
    "    percent_change = np.double(eng_score - lang_score)/np.double(eng_score)\n",
    "    return percent_change*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored, cprint\n",
    "import numpy as np\n",
    "\n",
    "f=open(\"dataset/genmodel_score.csv\", encoding='windows-1254');L=f.readlines();f.close()\n",
    "L_header = L[0].split(\",\")\n",
    "model_scores = dd(list)\n",
    "x=[]\n",
    "\n",
    "model_index_dict = {'GPT-3.5-Turbo Monolingual':10, 'GPT-3.5-Turbo Translate-Test':11, 'GPT-3.5-Turbo Zero-Shot Cross Lingual':12, \n",
    "               'GPT-4 Monolingual':13, \n",
    "                'GPT-4 Translate-Test':14, \n",
    "                'GPT-4 Zero-Shot Cross Lingual':15, \n",
    "                'TULRv6':16, \n",
    "                'BLOOMZ':17, \n",
    "                'XLM-R':18, \n",
    "                'mBERT':19, \n",
    "                'MuRIL':20, \n",
    "                'XGLM':21, \n",
    "                'mT5':22, \n",
    "                'PaLM2 Zero-Shot':23, \n",
    "                'PaLM2 Monolingual':24, \n",
    "                'Llama2-7B Monolingual':25, \n",
    "                'Llama2-13B Monolingual':26, \n",
    "                'Llama2-70B Monolingual':27, \n",
    "                'LLaVa-1.5-13B Monolingual':28, \n",
    "                'LLaVa-1.5-13B Translate-Test':29}\n",
    "\n",
    "model_to_eval = \"GPT-3.5-Turbo Zero-Shot Cross Lingual\"\n",
    "model_index = model_index_dict[model_to_eval]\n",
    "\n",
    "####\n",
    "for model_to_eval in model_index_dict.keys():\n",
    "    model_index = model_index_dict[model_to_eval]\n",
    "    for ele in L[1:]:\n",
    "        i=0\n",
    "        ele = ele.split(\",\")\n",
    "        class_id = class_lang[ele[2].strip()]\n",
    "        lang = ele[2].strip()\n",
    "        track = dataset_track[ele[0]].strip()\n",
    "        dataset = ele[0].strip()\n",
    "        #print(ele[model_index],class_id,lang,track,dataset,class_lang)\n",
    "        if lang in ['Assamese','Czech','Danish', 'Kashmiri','Norwegian', 'Swedish']:\n",
    "            continue\n",
    "        all_data[int(class_id[0])][lang][track][dataset] = ele[model_index]\n",
    "        \n",
    "\n",
    "    ##\n",
    "\n",
    "    '''\n",
    "    Trackwise English language scores for normalization\n",
    "    '''\n",
    "    english_norm_scores = dd()\n",
    "\n",
    "    for track in track_dataset:\n",
    "        #cprint(track,\"red\")\n",
    "        scores=[]\n",
    "        for dataset in all_data[5]['English'][track].keys():\n",
    "            #cprint(dataset,\"blue\")\n",
    "            #print(all_data[5]['English'][track][dataset])\n",
    "            if (all_data[5]['English'][track][dataset]) != \"NA\" and (all_data[5]['English'][track][dataset]) != \"\":\n",
    "                scores.append(np.double(all_data[5]['English'][track][dataset]))\n",
    "        if scores:\n",
    "            if np.sum(scores) > 0.0:\n",
    "                english_norm_scores[track] = np.mean([e for e in scores if e>0.0])\n",
    "                #print(\"English score\", english_norm_scores[track])\n",
    "                \n",
    "    #print(english_norm_scores)\n",
    "\n",
    "    '''\n",
    "    Use: english_norm_scores\n",
    "    '''\n",
    "\n",
    "    standardized_all_data = dd()\n",
    "    standardized_all_data_missed = dd()\n",
    "\n",
    "    for class_id in all_data.keys():\n",
    "        standardized_all_data[class_id]=dd()\n",
    "        standardized_all_data_missed[class_id]=dd()\n",
    "        for lang in all_data[class_id].keys():\n",
    "            if lang != \"English\":\n",
    "                standardized_all_data[class_id][lang]=dd()\n",
    "                standardized_all_data_missed[class_id][lang]=dd()\n",
    "                for track in english_norm_scores.keys():\n",
    "                    standardized_all_data[class_id][lang][track]=dd()\n",
    "                    standardized_all_data_missed[class_id][lang][track]=dd()\n",
    "                    for dataset in all_data[class_id][lang][track].keys():\n",
    "                        if (all_data[class_id][lang][track][dataset] !='NA') and (all_data[class_id][lang][track][dataset]!='') and np.double(all_data[class_id][lang][track][dataset]) > 0.0:\n",
    "                            standardized_all_data[class_id][lang][track][dataset] = get_percent_change(np.double(all_data[class_id][lang][track][dataset]), np.double(english_norm_scores[track]))\n",
    "                        else:\n",
    "                            standardized_all_data_missed[class_id][lang][track][dataset] = all_data[class_id][lang][track][dataset]\n",
    "                            #missing_data.append(class_id,lang,track,dataset, all_data[class_id][lang][track][dataset])\n",
    "                \n",
    "            #print(class_id, lang, standardized_all_data[class_id][lang])                        \n",
    "\n",
    "    class_wise_readiness_score = dd(list)\n",
    "    track_wise_readiness_score = dd(list)\n",
    "    for class_id in standardized_all_data.keys():\n",
    "        for lang in standardized_all_data[class_id].keys():\n",
    "            for track in standardized_all_data[class_id][lang].keys():\n",
    "                if standardized_all_data[class_id][lang][track].keys():\n",
    "                    for dataset in standardized_all_data[class_id][lang][track].keys():\n",
    "                        #print(class_id, lang, track, dataset, standardized_all_data[class_id][lang][track][dataset])\n",
    "                        class_wise_readiness_score[class_id].append(np.double(standardized_all_data[class_id][lang][track][dataset]))\n",
    "                        track_wise_readiness_score[track].append(np.double(standardized_all_data[class_id][lang][track][dataset]))\n",
    "\n",
    "    cprint(model_to_eval,\"red\")\n",
    "    print(class_wise_readiness_score)\n",
    "    for ele in class_wise_readiness_score.keys():\n",
    "        print(ele, np.mean(class_wise_readiness_score[ele]))\n",
    "        \n",
    "    print(track_wise_readiness_score)\n",
    "    for ele in track_wise_readiness_score.keys():\n",
    "        print(ele, np.mean(track_wise_readiness_score[ele]))\n",
    "        \n",
    "    print(\"Overall Score\");x=[]\n",
    "    for ele in class_wise_readiness_score.keys():\n",
    "        x.append(np.mean(class_wise_readiness_score[ele]))\n",
    "    print(np.mean(x))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
