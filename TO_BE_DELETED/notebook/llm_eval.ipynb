{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 3\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "import time\n",
    "import tqdm\n",
    "import ast\n",
    "import random\n",
    "import argparse\n",
    "import pdb\n",
    "from time import sleep\n",
    "import os\n",
    "\n",
    "# from utils import global_template, run_logger, task_list, load_copa, run_batched_logger, api_base, api_type, api_version, openai_key\n",
    "\n",
    "api_base = \"https://msri-openai-ifaq.azure-api.net\"\n",
    "api_type = \"azure\"\n",
    "api_version = \"2023-03-15-preview\"\n",
    "\n",
    "openai.api_base = api_base\n",
    "openai.api_type = api_type\n",
    "openai.api_version = api_version\n",
    "# openai.api_key = openai_key\n",
    "\n",
    "\n",
    "def attenuator(number_of_failed_hits):\n",
    "    return 2 ** (number_of_failed_hits)\n",
    "\n",
    "\n",
    "def construct_prompt(query, task):\n",
    "    message = []\n",
    "    language = query[\"lang\"]\n",
    "    sys_prompt = {\n",
    "        \"xlsum\": \"\"\"You are an LLM evaluator. Given the main text and the corresponding summary in {language}, your task is to rate the summary.\n",
    "    Rate the summary on a scale of 1 to 5, with 1 being the worst and 5 being the best. \\n\\n Just output the score and nothing else. \\n\\n\"\"\",\n",
    "        \"translation\": \"\"\"You are an LLM evaluator. Given the main text and the corresponding translation in {language}, your task is to rate the translation.\n",
    "    Rate the translation on a scale of 1 to 5, with 1 being the worst and 5 being the best. \\n\\nJust output the score and nothing else. \\n\\n\"\"\",\n",
    "    }\n",
    "    message.append({\"role\": \"system\", \"content\": f\"{sys_prompt[task]}\"})\n",
    "    message.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Main text: {query['text']}\\n Summary: {query['summ']}\\n\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return message\n",
    "\n",
    "\n",
    "def get_llm_eval(prompt):\n",
    "    number_of_failed_hits = 0\n",
    "    while number_of_failed_hits < 5:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                engine=\"gpt-35-turbo\", temperature=0, messages=prompt, max_tokens=100\n",
    "            )\n",
    "            output = response[\"choices\"][0][\"message\"][\"content\"].strip().split(\"\\n\")[0]\n",
    "            return output\n",
    "        except (openai.error.RateLimitError, openai.error.Timeout) as e:\n",
    "            output = \"FAILED\"\n",
    "            number_of_failed_hits += 1\n",
    "            slugger = attenuator(number_of_failed_hits)\n",
    "            print(f\"Retrying after {slugger} seconds!\")\n",
    "            sleep(slugger)\n",
    "        except (\n",
    "            openai.error.APIConnectionError,\n",
    "            openai.error.APIError,\n",
    "            openai.error.InvalidRequestError,\n",
    "            TypeError,\n",
    "        ) as e:\n",
    "            output = f\"FAILED with Unrecoverable Exception {e}.\"\n",
    "            break\n",
    "        except Exception as e:\n",
    "            output = f\"FAILED with generic exception {e}\"\n",
    "            break\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_seed_queries(seed_file):\n",
    "    with open(seed_file, \"r\") as f:\n",
    "        return f.read().split(\"\\n\")\n",
    "\n",
    "\n",
    "def batched_response(\n",
    "    task, queries, sleep_period, batch_size, temperature, response_logger_file\n",
    "):\n",
    "    batch_iter = 0\n",
    "    num_of_batches = len(queries) // batch_size\n",
    "    for batch_idx in range(num_of_batches):\n",
    "        batch_responses = []\n",
    "        for query in tqdm.tqdm(queries[batch_iter : batch_iter + batch_size]):\n",
    "            time.sleep(sleep_period)\n",
    "            prompt = construct_prompt(query, task)\n",
    "            model_response = get_llm_eval(prompt)\n",
    "            batch_responses.append(model_response)\n",
    "        # run_batched_logger(task, queries[batch_iter: batch_iter + batch_size], batch_responses, response_logger_file)\n",
    "        batch_iter += batch_size\n",
    "        print(f\"Processed {batch_idx + 1} batch. Moving on to next!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument('--root', type = str, default = '../results/llm_eval/')\n",
    "    # parser.add_argument('--model', type = str, default = \"gpt-35-turbo\") # Add a deployment name here\n",
    "    # parser.add_argument('--keypath', type = str, default = '../../openai_key.txt')\n",
    "    # parser.add_argument('--seed_dataset_name', type=str, default = 'xlsum')\n",
    "    # parser.add_argument('--sleep_period', type=float, default = 1)\n",
    "    # parser.add_argument('--batch_size', type = int, default = 25)\n",
    "    # parser.add_argument('--max_samples', type=int, default = 500)\n",
    "    # parser.add_argument('--temperature', type=float, default = 0)\n",
    "    # parser.add_argument('--queries_file', type=str, default = '../../data/EMNLP/explict_bias_seed.txt')\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    ## Initializing openai key and response logging file\n",
    "    # with open(args.keypath, 'r') as file:\n",
    "    #     openai.api_key = file.read().split('\\n')[0] # In case there are any useless delimiters\n",
    "\n",
    "    openai.api_key = \"\"\n",
    "    query = {}\n",
    "    query[\"text\"] = (\n",
    "        \"\"\"The Securities and Exchange Commission said it would \"review the unusual trading activity\" and \"take appropriate steps to protect investors\". There are rumours the drop may have been caused by an erroneous \"fat finger\" trade at a Wall Street bank. The term refers to when a trader enters data incorrectly. There was speculation that a Citigroup employee had been responsible for an erroneous trade - however, the bank said there was no evidence of this. \"We, along with the rest of the financial industry, are investigating to find the source of today's market volatility,\" Citigroup spokesman Stephen Cohen said in a statement. \"At this point, we have no evidence that Citi was involved in any erroneous transaction.\" US President Barack Obama said the authorities were evaluating the unusual activity and hoped to prevent it from happening again. \"They will make findings of their review public along with recommendations for appropriate action,\" he told reporters on Friday. Algorithms Among those affected were 3M, whose shares fell 25t one point, and Procter & Gamble, which saw its stocks fall by 37%. There have been reports that a \"fat finger\" incident involved trade in Procter & Gamble shares, when a trader entered a \"b\" for billion instead of \"m\" for million. All share trading is processed electronically - with stocks changing hands in fractions of a second. One theory is that algorithms on which systems rely automatically triggered thousands of transactions to be executed after an error was made. The tumultuous session on Wall Street saw the Dow Jones index fall by almost 1,000 points in a matter of minutes. It later recovered, closing down 3.2%, or 347.8 points, at 10,520.32. The Nasdaq exchange also saw sharp drops and said it was also investigating what happened.\"\"\"\n",
    "    )\n",
    "    query[\"summ\"] = (\n",
    "        \"\"\"Stock market go down because of fat finger mistake, maybe bank do bad trade. People confused, stocks change hands fast. Wall Street and SEC looking into problem.\"\"\"\n",
    "    )\n",
    "    query[\"lang\"] = \"en\"\n",
    "\n",
    "    # response_logger_file = f'{args.root}{args.seed_dataset_name}.txt'\n",
    "    response_logger_file = \"results.txt\"\n",
    "    prompt = construct_prompt(query, \"xlsum\")\n",
    "    model_response = get_llm_eval(prompt)\n",
    "    print(model_response)\n",
    "    # batched_response('xlsum', query, 1, 1, 0,response_logger_file)\n",
    "\n",
    "    # if args.queries_file:\n",
    "    #     with open(args.queries_file, 'r') as file:\n",
    "    #         queries = file.read().strip().split('\\n')[:args.max_samples]\n",
    "    #         batched_response(args.task, queries,  args.sleep_period, args.batch_size, args.temperature, response_logger_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
