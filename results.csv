Dataset,Language,Metric,gemma-2b-it,Strategy,gemma-7b-it,gemini-pro
afriqa,fon,exact_match,0.0,PivotLang_fon_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,
afriqa,fon,f1,0.0,PivotLang_fon_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,
afriqa,hau,exact_match,0.0,PivotLang_hau_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,0.0,
afriqa,hau,f1,0.0,PivotLang_hau_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,0.38647342995169087,
afriqa,ibo,exact_match,0.0,PivotLang_ibo_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,0.0,
afriqa,ibo,f1,0.0,PivotLang_ibo_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,0.0,
afriqa,swa,exact_match,0.0,PivotLang_swa_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,
afriqa,swa,f1,0.38941798941798944,PivotLang_swa_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,
afriqa,zul,exact_match,0.0,PivotLang_zul_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,0.0,
afriqa,zul,f1,0.0,PivotLang_zul_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,0.0,
belebele,arabic,accuracy,0.3611111111111111,PivotLang_arabic_PromptName_Choose the correct answer_FewShotK_0,0.45666666666666667,0.89
belebele,chinese_simplified,accuracy,0.4122222222222222,PivotLang_chinese_simplified_PromptName_Choose the correct answer_FewShotK_0,0.5766666666666667,0.9144444444444444
belebele,chinese_traditional,accuracy,0.39222222222222225,PivotLang_chinese_traditional_PromptName_Choose the correct answer_FewShotK_0,0.5711111111111111,0.9033333333333333
belebele,czech,accuracy,0.35,PivotLang_czech_PromptName_Choose the correct answer_FewShotK_0,0.5388888888888889,0.9044444444444445
belebele,danish,accuracy,0.34,PivotLang_danish_PromptName_Choose the correct answer_FewShotK_0,0.5088888888888888,0.8977777777777778
belebele,dutch,accuracy,0.35888888888888887,PivotLang_dutch_PromptName_Choose the correct answer_FewShotK_0,0.4911111111111111,0.8888888888888888
belebele,english,accuracy,0.4222222222222222,PivotLang_english_PromptName_Choose the correct answer_FewShotK_0,0.6633333333333333,0.9077777777777778
belebele,finnish,accuracy,0.3477777777777778,PivotLang_finnish_PromptName_Choose the correct answer_FewShotK_0,0.4777777777777778,0.8933333333333333
belebele,french,accuracy,0.4,PivotLang_french_PromptName_Choose the correct answer_FewShotK_0,0.5666666666666667,0.8977777777777778
belebele,german,accuracy,0.3466666666666667,PivotLang_german_PromptName_Choose the correct answer_FewShotK_0,0.5533333333333333,0.8877777777777778
belebele,hebrew,accuracy,0.31666666666666665,PivotLang_hebrew_PromptName_Choose the correct answer_FewShotK_0,0.44666666666666666,0.88
belebele,hungarian,accuracy,0.2911111111111111,PivotLang_hungarian_PromptName_Choose the correct answer_FewShotK_0,0.46,0.8822222222222222
belebele,italian,accuracy,0.37333333333333335,PivotLang_italian_PromptName_Choose the correct answer_FewShotK_0,0.5633333333333334,0.8933333333333333
belebele,japanese,accuracy,0.35777777777777775,PivotLang_japanese_PromptName_Choose the correct answer_FewShotK_0,0.47444444444444445,0.8688888888888889
belebele,korean,accuracy,0.36444444444444446,PivotLang_korean_PromptName_Choose the correct answer_FewShotK_0,0.46,0.8788888888888889
belebele,norwegian,accuracy,0.34,PivotLang_norwegian_PromptName_Choose the correct answer_FewShotK_0,0.5344444444444445,0.8877777777777778
belebele,polish,accuracy,0.35,PivotLang_polish_PromptName_Choose the correct answer_FewShotK_0,0.5111111111111111,0.8811111111111111
belebele,portuguese,accuracy,0.27666666666666667,PivotLang_portuguese_PromptName_Choose the correct answer_FewShotK_0,0.5844444444444444,0.8944444444444445
belebele,russian,accuracy,0.4,PivotLang_russian_PromptName_Choose the correct answer_FewShotK_0,0.5622222222222222,0.8911111111111111
belebele,spanish,accuracy,0.39222222222222225,PivotLang_spanish_PromptName_Choose the correct answer_FewShotK_0,0.5777777777777777,0.8933333333333333
belebele,swedish,accuracy,0.3622222222222222,PivotLang_swedish_PromptName_Choose the correct answer_FewShotK_0,0.5255555555555556,0.8888888888888888
belebele,thai,accuracy,0.36666666666666664,PivotLang_thai_PromptName_Choose the correct answer_FewShotK_0,0.47555555555555556,0.8344444444444444
belebele,turkish,accuracy,0.32,PivotLang_turkish_PromptName_Choose the correct answer_FewShotK_0,0.41444444444444445,0.8411111111111111
gluecos_nli,english-hindi,accuracy,,PivotLang_en_PromptName_GPT-3 style_FewShotK_8,,0.8080357142857143
gluecos_sentiment,english-hindi,accuracy,,PivotLang_en_PromptName_following positive negative neutral_FewShotK_8,,0.2943722943722944
indicqa,as,exact,0.11179429849077697,PivotLang_as_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,3.0184460592509783,
indicqa,as,exact,5.477920626048071,PivotLang_as_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,12.57685858021241,
indicqa,as,f1,2.0375346101981755,PivotLang_as_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,8.332517268674053,
indicqa,as,f1,6.604766096252523,PivotLang_as_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,19.075903239309742,
indicqa,bn,exact,0.45377197958026094,PivotLang_bn_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,4.367555303460011,
indicqa,bn,exact,2.949517867271696,PivotLang_bn_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,14.917753828701077,
indicqa,bn,f1,2.775239251220343,PivotLang_bn_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,10.043341850844282,
indicqa,bn,f1,4.477752739335431,PivotLang_bn_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,21.92565700003274,
indicqa,gu,exact,0.7436787307882995,PivotLang_gu_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,3.668815071888944,
indicqa,gu,exact,9.568666336142787,PivotLang_gu_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,20.079325731284086,
indicqa,gu,f1,1.4994200203489023,PivotLang_gu_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,5.349166009965299,
indicqa,gu,f1,10.095029527237342,PivotLang_gu_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,22.18160971649274,
indicqa,hi,exact,0.7110536522301228,PivotLang_hi_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,9.95475113122172,
indicqa,hi,exact,1.8099547511312217,PivotLang_hi_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,21.78409825468649,
indicqa,hi,f1,5.266714775652699,PivotLang_hi_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,18.44930354473504,
indicqa,hi,f1,5.867592600469709,PivotLang_hi_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,31.9597649419029,
indicqa,kn,exact,1.911667765326302,PivotLang_kn_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,4.878048780487805,
indicqa,kn,exact,11.60184574818721,PivotLang_kn_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,17.402768622280817,
indicqa,kn,f1,3.352141989351476,PivotLang_kn_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,8.587138217568194,
indicqa,kn,f1,12.649509016810528,PivotLang_kn_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,21.843809549087823,
indicqa,ml,exact,0.6293266205160478,PivotLang_ml_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,1.8250471994965387,
indicqa,ml,exact,2.9578351164254246,PivotLang_ml_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,12.64946507237256,
indicqa,ml,f1,2.0362274850609596,PivotLang_ml_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,3.7548455937574863,
indicqa,ml,f1,4.020313408991595,PivotLang_ml_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,15.550442718533288,
indicqa,mr,exact,0.18703241895261846,PivotLang_mr_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,5.548628428927681,
indicqa,mr,exact,3.9276807980049875,PivotLang_mr_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,16.39650872817955,
indicqa,mr,f1,2.8140253820874097,PivotLang_mr_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,11.685667511641803,
indicqa,mr,f1,5.488536642017987,PivotLang_mr_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,22.547266952679024,
indicqa,or,exact,0.0,PivotLang_or_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,0.05952380952380952,
indicqa,or,exact,2.261904761904762,PivotLang_or_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,7.440476190476191,
indicqa,or,f1,0.008073344280240832,PivotLang_or_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,0.06362889983579638,
indicqa,or,f1,2.2668650793650795,PivotLang_or_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,7.4867724867724865,
indicqa,pa,exact,0.648508430609598,PivotLang_pa_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,3.9559014267185475,
indicqa,pa,exact,5.058365758754864,PivotLang_pa_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,15.045395590142672,
indicqa,pa,f1,4.098676375300749,PivotLang_pa_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,7.062110930610076,
indicqa,pa,f1,7.669178992819603,PivotLang_pa_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,19.41645737374189,
indicqa,ta,exact,1.3303769401330376,PivotLang_ta_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,7.150776053215077,
indicqa,ta,exact,2.3835920177383594,PivotLang_ta_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,18.126385809312637,
indicqa,ta,f1,5.621336949244624,PivotLang_ta_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,12.644818439615312,
indicqa,ta,f1,5.622540783470951,PivotLang_ta_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,24.288613919110663,
indicqa,te,exact,0.7497116493656286,PivotLang_te_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,4.901960784313726,
indicqa,te,exact,2.5374855824682814,PivotLang_te_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,12.629757785467127,
indicqa,te,f1,2.247414753638631,PivotLang_te_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4_with_unanswerable,6.67702185798376,
indicqa,te,f1,3.3865665853113955,PivotLang_te_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8_with_unanswerable,15.193993447981905,
mlqa,ar,exact_match,,PivotLang_ar_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,21.663442940038685
mlqa,ar,exact_match,6.963249516441006,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,16.054158607350097,
mlqa,ar,f1,,PivotLang_ar_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,38.934689066857736
mlqa,ar,f1,16.471464676233794,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,30.286783717488838,
mlqa,de,exact_match,,PivotLang_de_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,46.2890625
mlqa,de,exact_match,16.015625,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,40.0390625,
mlqa,de,f1,,PivotLang_de_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,61.40875899832756
mlqa,de,f1,31.135639307026057,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,56.0475258201017,
mlqa,en,exact_match,20.73170731707317,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,53.48432055749129,66.37630662020906
mlqa,en,f1,43.37233115173211,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,71.98988799336566,78.23557140954983
mlqa,es,exact_match,17.2,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,34.2,
mlqa,es,exact_match,,PivotLang_es_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,48.4
mlqa,es,f1,36.599534336349436,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,58.391420460601466,
mlqa,es,f1,,PivotLang_es_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,69.88878043855512
mlqa,hi,exact_match,8.678500986193294,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,24.45759368836292,
mlqa,hi,exact_match,,PivotLang_hi_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,47.337278106508876
mlqa,hi,f1,18.403136506138384,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,40.72727925491323,
mlqa,hi,f1,,PivotLang_hi_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,63.25899525988124
mlqa,vi,exact_match,13.894324853228962,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,38.35616438356164,
mlqa,vi,exact_match,,PivotLang_vi_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,51.2720156555773
mlqa,vi,f1,32.01221308482639,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,60.548900317799884,
mlqa,vi,f1,,PivotLang_vi_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,70.97429090474996
mlqa,zh,exact_match,1.5873015873015872,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,5.555555555555555,
mlqa,zh,exact_match,,PivotLang_zh_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,7.5396825396825395
mlqa,zh,f1,5.102773042963584,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,9.058151647275317,
mlqa,zh,f1,,PivotLang_zh_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,10.322184429327285
panx,ar,f1-score,,PivotLang_ar_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.47793190416141235
panx,bg,f1-score,,PivotLang_bg_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.4533969010727056
panx,bn,f1-score,,PivotLang_bn_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.4176920518789491
panx,de,f1-score,,PivotLang_de_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.34989754098360654
panx,el,f1-score,,PivotLang_el_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.46729957805907174
panx,en,f1-score,,PivotLang_en_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.3413437959784208
panx,es,f1-score,,PivotLang_es_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.3450356555128909
panx,et,f1-score,,PivotLang_et_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.44318488529014843
panx,fi,f1-score,,PivotLang_fi_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.4633077765607886
panx,fr,f1-score,,PivotLang_fr_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.40594625500285875
panx,he,f1-score,,PivotLang_he_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.3784870570495099
panx,hi,f1-score,,PivotLang_hi_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.4515727187791965
panx,hu,f1-score,,PivotLang_hu_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.2503646086533787
panx,id,f1-score,,PivotLang_id_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.4614446814995428
panx,it,f1-score,,PivotLang_it_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.5339805825242718
panx,ja,f1-score,,PivotLang_ja_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.06074916818718472
panx,ko,f1-score,,PivotLang_ko_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.3569903948772678
panx,lt,f1-score,,PivotLang_lt_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.5502793296089387
panx,nl,f1-score,,PivotLang_nl_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.5404268109407876
panx,pl,f1-score,,PivotLang_pl_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.49337300513930216
panx,pt,f1-score,,PivotLang_pt_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.4512735326688815
panx,ro,f1-score,,PivotLang_ro_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.4065533980582525
panx,ru,f1-score,,PivotLang_ru_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.38753959873284055
panx,sw,f1-score,,PivotLang_sw_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.5748373101952277
panx,th,f1-score,,PivotLang_th_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.011071922853698825
panx,tr,f1-score,,PivotLang_tr_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.5611333538651062
panx,uk,f1-score,,PivotLang_uk_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.3769520732364028
panx,vi,f1-score,,PivotLang_vi_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.47553461399057634
panx,zh,f1-score,,PivotLang_zh_PromptName_structure_prompting_chat_wth_instruct_Verbalizer_identity_FewShotK_8wthInstruction,,0.07898032859353128
pawsx,de,accuracy,0.548,PivotLang_de_PromptName_PAWS-ANLI GPT3_FewShotK_8,0.516,0.768688293370945
pawsx,en,accuracy,,PivotLang_en_PromptName_PAWS-ANLI GPT3_FewShotK_8,0.579,0.8004569687738005
pawsx,es,accuracy,0.5385,PivotLang_es_PromptName_PAWS-ANLI GPT3_FewShotK_8,0.57,0.7643821910955477
pawsx,fr,accuracy,0.5495,PivotLang_fr_PromptName_PAWS-ANLI GPT3_FewShotK_8,0.5235,0.7566938300349243
pawsx,ja,accuracy,,PivotLang_ja_PromptName_PAWS-ANLI GPT3_FewShotK_8,0.456,0.6735
pawsx,ko,accuracy,,PivotLang_ko_PromptName_PAWS-ANLI GPT3_FewShotK_8,0.458,0.6575
pawsx,zh,accuracy,0.5385,PivotLang_zh_PromptName_PAWS-ANLI GPT3_FewShotK_8,0.4835,0.724
tydiqa,ar,exact_match,15.852334419109663,PivotLang_ar_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,37.56786102062975,54.071661237785015
tydiqa,ar,f1,43.60322391634344,PivotLang_ar_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,61.726502127934914,63.72921086891905
tydiqa,bn,exact_match,4.424778761061947,PivotLang_bn_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4,32.743362831858406,66.3716814159292
tydiqa,bn,f1,20.504603557700896,PivotLang_bn_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_4,52.12766338872534,75.08159981611308
tydiqa,en,exact_match,29.545454545454547,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,51.59090909090909,59.31818181818182
tydiqa,en,f1,51.855509519499265,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,68.22985505811455,71.21906847676753
tydiqa,fi,exact_match,30.81841432225064,PivotLang_fi_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,53.452685421994886,60.74168797953964
tydiqa,fi,f1,49.131214199767705,PivotLang_fi_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,68.01144372255061,68.79280208635494
tydiqa,id,exact_match,20.353982300884955,PivotLang_id_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,53.45132743362832,64.24778761061947
tydiqa,id,f1,43.60088536733675,PivotLang_id_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,71.33322865414297,75.04043137878185
tydiqa,ko,exact_match,22.82608695652174,PivotLang_ko_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,52.53623188405797,65.57971014492753
tydiqa,ko,f1,39.35981117746365,PivotLang_ko_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,64.52252604017644,73.96319959883289
tydiqa,ru,exact_match,15.64039408866995,PivotLang_ru_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,31.40394088669951,48.891625615763544
tydiqa,ru,f1,39.65796411194289,PivotLang_ru_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,52.513471786947164,60.846524601697034
tydiqa,sw,exact_match,44.68937875751503,PivotLang_sw_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,62.92585170340681,73.14629258517034
tydiqa,sw,f1,57.41356824405606,PivotLang_sw_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,75.13675006938752,79.74718254482994
xcopa,en,accuracy,0.61,PivotLang_en_PromptName_plausible_alternatives_discrete_FewShotK_8,,0.99
xcopa,et,accuracy,0.5,PivotLang_et_PromptName_plausible_alternatives_discrete_FewShotK_8_use_val_to_prompt,0.6060606060606061,0.962
xcopa,ht,accuracy,0.5,PivotLang_ht_PromptName_plausible_alternatives_discrete_FewShotK_8_use_val_to_prompt,0.486,
xcopa,id,accuracy,0.5,PivotLang_id_PromptName_plausible_alternatives_discrete_FewShotK_8_use_val_to_prompt,0.6923076923076923,0.958
xcopa,it,accuracy,0.512,PivotLang_it_PromptName_plausible_alternatives_discrete_FewShotK_8_use_val_to_prompt,0.738,0.976
xcopa,qu,accuracy,0.5,PivotLang_qu_PromptName_plausible_alternatives_discrete_FewShotK_8_use_val_to_prompt,0.496,
xcopa,sw,accuracy,0.5,PivotLang_sw_PromptName_plausible_alternatives_discrete_FewShotK_8_use_val_to_prompt,,0.9
xcopa,ta,accuracy,0.5,PivotLang_ta_PromptName_plausible_alternatives_discrete_FewShotK_8_use_val_to_prompt,0.506,
xcopa,th,accuracy,0.498,PivotLang_th_PromptName_plausible_alternatives_discrete_FewShotK_8_use_val_to_prompt,0.614,0.95
xcopa,tr,accuracy,0.5127272727272727,PivotLang_tr_PromptName_plausible_alternatives_discrete_FewShotK_8_use_val_to_prompt,,0.96
xcopa,vi,accuracy,0.506,PivotLang_vi_PromptName_plausible_alternatives_discrete_FewShotK_8_use_val_to_prompt,0.728,
xcopa,zh,accuracy,0.514,PivotLang_zh_PromptName_plausible_alternatives_discrete_FewShotK_8_use_val_to_prompt,0.814,
xnli,ar,accuracy,0.48522954091816367,PivotLang_ar_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.46407185628742514,0.6784178847807395
xnli,as,accuracy,0.3898203592814371,PivotLang_as_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.4435129740518962,
xnli,bg,accuracy,0.4786427145708583,PivotLang_bg_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.5227544910179641,0.7502994011976047
xnli,bn,accuracy,0.40838323353293415,PivotLang_bn_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.4746506986027944,
xnli,de,accuracy,0.4546906187624751,PivotLang_de_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.5263473053892216,0.7578842315369262
xnli,el,accuracy,0.47065868263473054,PivotLang_el_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.49820359281437127,0.7271457085828343
xnli,en,accuracy,0.5069860279441117,PivotLang_en_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.5692614770459082,0.7904191616766467
xnli,es,accuracy,0.4692614770459082,PivotLang_es_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.5203592814371257,0.7646706586826347
xnli,fr,accuracy,0.4419161676646707,PivotLang_fr_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.5269461077844312,0.7351297405189621
xnli,gu,accuracy,0.41357285429141716,PivotLang_gu_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.4530938123752495,
xnli,hi,accuracy,0.4846307385229541,PivotLang_hi_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.45728542914171655,0.6808383233532934
xnli,kn,accuracy,0.39481037924151696,PivotLang_kn_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.46187624750499,
xnli,ml,accuracy,0.3998003992015968,PivotLang_ml_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.4626746506986028,
xnli,mr,accuracy,0.40778443113772456,PivotLang_mr_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.43832335329341315,
xnli,or,accuracy,0.3287425149700599,PivotLang_or_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.33652694610778444,
xnli,pa,accuracy,0.40858283433133735,PivotLang_pa_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.45828343313373254,
xnli,ru,accuracy,0.49860279441117766,PivotLang_ru_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.5043912175648703,0.7225548902195609
xnli,sw,accuracy,0.38902195608782436,PivotLang_sw_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.43313373253493015,0.6708582834331337
xnli,ta,accuracy,0.4119760479041916,PivotLang_ta_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.4564870259481038,
xnli,te,accuracy,0.42075848303393215,PivotLang_te_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.45768463073852295,
xnli,th,accuracy,0.4666666666666667,PivotLang_th_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.4872255489021956,0.6203592814371257
xnli,tr,accuracy,0.4528942115768463,PivotLang_tr_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.468063872255489,0.7223552894211577
xnli,ur,accuracy,0.4499001996007984,PivotLang_ur_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.43832335329341315,
xnli,vi,accuracy,0.49600798403193613,PivotLang_vi_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.48982035928143713,0.6790419161676646
xnli,zh,accuracy,0.5239520958083832,PivotLang_zh_PromptName_GPT-3 style_FewShotK_8_temperature_0.0,0.474251497005988,0.6614770459081837
xquad,ar,exact_match,,PivotLang_ar_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,52.6890756302521
xquad,ar,exact_match,2.1848739495798317,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,24.285714285714285,
xquad,ar,f1,,PivotLang_ar_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,68.76674812792218
xquad,ar,f1,14.813341366385895,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,42.39113640109605,
xquad,de,exact_match,,PivotLang_de_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,56.72268907563025
xquad,de,exact_match,4.873949579831932,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,41.84873949579832,
xquad,de,f1,,PivotLang_de_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,73.7378409648832
xquad,de,f1,18.20620883191606,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,60.57510672701298,
xquad,el,exact_match,,PivotLang_el_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,52.35294117647059
xquad,el,exact_match,6.974789915966387,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,28.319327731092436,
xquad,el,f1,,PivotLang_el_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,67.88221166706082
xquad,el,f1,17.677466343739315,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,44.412915553358566,
xquad,en,exact_match,6.80672268907563,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,56.30252100840336,72.18487394957984
xquad,en,f1,24.004768196661313,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,73.27447744319012,83.14090879619806
xquad,es,exact_match,3.361344537815126,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,38.15126050420168,
xquad,es,exact_match,,PivotLang_es_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,59.57983193277311
xquad,es,f1,19.659572393734752,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,64.55143770973588,
xquad,es,f1,,PivotLang_es_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,77.1431174403784
xquad,hi,exact_match,3.0252100840336134,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,25.630252100840337,
xquad,hi,exact_match,,PivotLang_hi_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,57.226890756302524
xquad,hi,f1,10.805965028412439,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,41.41143726015697,
xquad,hi,f1,,PivotLang_hi_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,71.5908205812497
xquad,ro,exact_match,2.689075630252101,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,41.34453781512605,
xquad,ro,exact_match,,PivotLang_ro_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,62.18487394957983
xquad,ro,f1,17.757018853974458,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,59.88456261777742,
xquad,ro,f1,,PivotLang_ro_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,75.22855369650414
xquad,ru,exact_match,2.6050420168067228,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,31.176470588235293,
xquad,ru,exact_match,,PivotLang_ru_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,47.3109243697479
xquad,ru,f1,14.585814005454404,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,51.41966264115091,
xquad,ru,f1,,PivotLang_ru_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,62.90514938011452
xquad,th,exact_match,9.327731092436975,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,29.66386554621849,
xquad,th,exact_match,,PivotLang_th_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,62.773109243697476
xquad,th,f1,17.628996202883854,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,42.17401240805122,
xquad,th,f1,,PivotLang_th_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,68.8569166594376
xquad,tr,exact_match,3.0252100840336134,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,23.19327731092437,
xquad,tr,exact_match,,PivotLang_tr_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,51.42857142857143
xquad,tr,f1,15.00463466808035,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,44.66474186925313,
xquad,tr,f1,,PivotLang_tr_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,67.85947993976929
xquad,vi,exact_match,4.873949579831932,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,41.51260504201681,
xquad,vi,exact_match,,PivotLang_vi_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,59.411764705882355
xquad,vi,f1,22.864887852601537,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,64.69925189579946,
xquad,vi,f1,,PivotLang_vi_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,77.02384564539214
xquad,zh,exact_match,4.453781512605042,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,33.36134453781513,
xquad,zh,exact_match,,PivotLang_zh_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,55.88235294117647
xquad,zh,f1,12.51940506436116,PivotLang_en_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,40.496033514187424,
xquad,zh,f1,,PivotLang_zh_PromptName_answer_given_context_and_question_Verbalizer_identity_FewShotK_8,,59.41890089369078
xstory_cloze,ar,accuracy,0.6465916611515553,PivotLang_ar_PromptName_Answer Given options_Verbalizer_identity_FewShotK_8wthInstruction,,
xstory_cloze,en,accuracy,0.7657180675049636,PivotLang_en_PromptName_Answer Given options_Verbalizer_identity_FewShotK_8wthInstruction,,
xstory_cloze,es,accuracy,0.6909331568497684,PivotLang_es_PromptName_Answer Given options_Verbalizer_identity_FewShotK_8wthInstruction,0.9073461283917935,
xstory_cloze,eu,accuracy,0.5307743216412971,PivotLang_eu_PromptName_Answer Given options_Verbalizer_identity_FewShotK_8wthInstruction,,
xstory_cloze,hi,accuracy,0.642620780939775,PivotLang_hi_PromptName_Answer Given options_Verbalizer_identity_FewShotK_8wthInstruction,0.7207147584381205,
xstory_cloze,id,accuracy,0.6591661151555261,PivotLang_id_PromptName_Answer Given options_Verbalizer_identity_FewShotK_8wthInstruction,,
xstory_cloze,my,accuracy,0.5579086697551291,PivotLang_my_PromptName_Answer Given options_Verbalizer_identity_FewShotK_8wthInstruction,,
xstory_cloze,ru,accuracy,0.6532097948378557,PivotLang_ru_PromptName_Answer Given options_Verbalizer_identity_FewShotK_8wthInstruction,,
xstory_cloze,sw,accuracy,0.5466578424884183,PivotLang_sw_PromptName_Answer Given options_Verbalizer_identity_FewShotK_8wthInstruction,,
xstory_cloze,te,accuracy,0.5440105890138981,PivotLang_te_PromptName_Answer Given options_Verbalizer_identity_FewShotK_8wthInstruction,0.4109861019192588,
xstory_cloze,zh,accuracy,0.7107875579086698,PivotLang_zh_PromptName_Answer Given options_Verbalizer_identity_FewShotK_8wthInstruction,0.8669755129053607,
